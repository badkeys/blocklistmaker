#!/usr/bin/python3

import argparse
import binascii
import datetime
import hashlib
import json
import os
import pathlib
import shutil
import subprocess
import sys

import cryptography
from cryptography.hazmat import backends
from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.primitives.asymmetric import dh, dsa, ec, ed448, ed25519, rsa, x448, x25519


def hashnumber(i):
    i_bin = i.to_bytes((i.bit_length() + 7) // 8, byteorder="big")
    return hashlib.sha256(i_bin).hexdigest()[:30]


def dummy(_a, _b):
    return 1


def msg(msg):
    if not args.quiet:
        print(msg)


def getlatest(fp):
    mt = int(os.path.getmtime(fp))
    for root, dirs, _ in os.walk(fp):
        for dp in dirs:
            sd = os.scandir(f"{root}/{dp}")
            mt = int(max([mt, *[f.stat().st_mtime for f in sd]]))
    return mt


def makebl(blid, source, dest, fileext, cache=False):
    os.mkdir(dest)
    flog = open(f"{dest}/blocklist.log", "w", encoding="ascii")

    keys = {}
    for root, dirs, files in os.walk(source, followlinks=True):
        if root.endswith("/.git"):
            msg(f"skipping .git ({root})")
            del dirs[:]
            continue
        sroot = root.removeprefix(source).removeprefix("/")
        msg(f"processing {root}")
        # sorting for deterministic output
        dirs.sort()
        for fn in sorted(files):
            if not fn.endswith(fileext):
                continue
            if sroot == "":
                fpath = f"{fn}"
            else:
                fpath = f"{sroot}/{fn}"
            key = pathlib.Path(f"{root}/{fn}").read_bytes()
            try:
                privkey = serialization.load_pem_private_key(
                    key,
                    password=None,
                    backend=mybackend,
                    unsafe_skip_rsa_key_validation=True,
                )
            except TypeError:  # no password
                flog.write(f"{root}/{fn} pem no password\n")
                continue
            except cryptography.exceptions.UnsupportedAlgorithm:  # e.g. PSS
                flog.write(f"{root}/{fn} pem unsupported algorithm\n")
                continue
            except ValueError:
                continue

            pubkey = privkey.public_key()

            if isinstance(pubkey, rsa.RSAPublicKey):
                hval = pubkey.public_numbers().n

            elif isinstance(pubkey, ec.EllipticCurvePublicKey):
                hval = pubkey.public_numbers().x

            elif isinstance(pubkey, (ed25519.Ed25519PublicKey, x25519.X25519PublicKey,
                            x448.X448PublicKey, ed448.Ed448PublicKey)):
                # "raw" ed25519 keys are simply the x coordinate
                ec_xb = pubkey.public_bytes(
                    encoding=serialization.Encoding.Raw,
                    format=serialization.PublicFormat.Raw
                )
                hval = int.from_bytes(ec_xb, byteorder="big")

            elif isinstance(pubkey, (dsa.DSAPublicKey, dh.DHPublicKey)):
                try:
                    hval = pubkey.public_numbers().y
                except ValueError:
                    # happens with implausibly small parameters
                    pass

            else:
                flog.write(f"{root}/{fn} unknown type {type(pubkey)}\n")
                msg(f"{root}/{fn} unknown type {type(pubkey)}")
                continue

            s256 = hashnumber(hval)
            keys[s256] = [blid, fpath]

    flog.close()

    if cache:
        with open(f"{dest}/keys.json", "w", encoding="ascii") as fj:
            json.dump(keys, fj)
    else:
        fs = open(f"{dest}/keys-{blid}-short.txt", "w", encoding="ascii")
        fl = open(f"{dest}/keys-{blid}-long.txt", "w", encoding="ascii")

        for k in sorted(keys):
            fs.write(f"{k}{keys[k][0]:02x}\n")
            fl.write(f"{k[0:16]};{keys[k][1]}\n")

        fs.close()
        fl.close()


def makefromjson(jsoninput, destdir, keyrepos, fileext):
    with open(jsoninput, encoding="ascii") as ft:
        template = json.load(ft)

    cachepath = os.path.expanduser("~/.cache/blocklistmaker")
    if not os.path.exists(cachepath):
        os.makedirs(cachepath)

    msg("Checking repos and creating caches...")
    caches = set()
    latest = 0
    nolook = []
    repodata = {}
    for bl in template["blocklists"]:
        shortname = bl["repo"].split("/")[1]
        rpath = os.path.join(os.path.expanduser(keyrepos), shortname)
        if bl["type"] == "new":
            if not os.path.exists(rpath):
                msg(f"Could not find {rpath}")
                continue
            # for plain directories, use latest mtime as cacheid
            nolook.append(bl["id"])
            cacheid = getlatest(rpath)
            latest = max(latest, cacheid)

        elif bl["type"] == "github":
            if shortname not in repodata:
                if not os.path.exists(rpath):
                    gurl = f"https://github.com/{bl['repo']}"
                    subprocess.check_output(["git", "clone", "-q", "--depth", "1", gurl, rpath])
                else:
                    subprocess.check_output(["git", "-C", rpath, "pull", "-q"])
                # for git repos, use latest commit hash as cacheid
                gid = subprocess.check_output(["git", "-C", rpath, "rev-parse", "HEAD"])
                repodata[shortname] = gid.decode().strip()
                gtime = subprocess.check_output(["git", "-C", rpath, "-P", "log", "-1",
                                                 "--format=%ct"])
                latest = max(latest, int(gtime))
            cacheid = repodata[shortname]

        cachedir = f"{shortname}-{bl['id']}-{cacheid}"
        if not os.path.exists(f"{cachepath}/{cachedir}"):
            if "/" in bl["path"]:
                rxpath = os.path.join(rpath, bl["path"].split("/", 1)[1])
            else:
                rxpath = rpath
            makebl(bl["id"], rxpath, f"{cachepath}/{cachedir}.tmp", fileext, cache=True)
            os.rename(f"{cachepath}/{cachedir}.tmp", f"{cachepath}/{cachedir}")

        caches.add(cachedir)

    latestiso = datetime.datetime.fromtimestamp(latest, datetime.UTC).isoformat()

    cachedirlist = set(os.listdir(cachepath))
    for rm in (cachedirlist - caches):
        msg(f"Removing {rm} (obsolete) from cache")
        shutil.rmtree(f"{cachepath}/{rm}")

    outdir = os.path.join(os.path.expanduser(destdir), str(latest))
    if os.path.exists(outdir):
        msg(f"{outdir} exists, nothing to do...")
        sys.exit(0)
    tmpout = f"{outdir}.tmp"
    os.makedirs(tmpout)

    msg("Merging caches...")
    keys = {}
    for cache in caches:
        with open(f"{cachepath}/{cache}/keys.json", encoding="ascii") as fj:
            j = json.load(fj)
        keys = j | keys

    msg("Writing output...")
    dat = []
    look = []
    for k in sorted(keys):
        blid = keys[k][0]
        dat.append(f"{k}{blid:02x}")
        if blid not in nolook:
            look.append(f"{k[0:16]};{keys[k][1]}\n")

    dat = "".join(dat)
    look = "".join(look)

    bdat = binascii.unhexlify(dat)
    blook = look.encode(encoding="ascii")
    pathlib.Path(f"{tmpout}/blocklist.dat").write_bytes(bdat)
    pathlib.Path(f"{tmpout}/lookup.txt").write_bytes(blook)

    # command-line xz is faster than python's lzma module
    subprocess.check_output(["xz", f"{tmpout}/blocklist.dat"])
    subprocess.check_output(["xz", f"{tmpout}/lookup.txt"])

    template["blocklist_sha256"] = hashlib.sha256(bdat).hexdigest()
    template["lookup_sha256"] = hashlib.sha256(blook).hexdigest()
    template["date"] = latestiso
    template["blocklist_url"] = template["blocklist_url"].replace("__TIMESTAMP__", str(latest))
    template["lookup_url"] = template["lookup_url"].replace("__TIMESTAMP__", str(latest))

    with open(f"{tmpout}/badkeysdata.json", "w", encoding="ascii") as fj:
        json.dump(template, fj, indent=2)
        fj.write("\n")

    os.rename(tmpout, outdir)

    lfile = os.path.join(os.path.expanduser(destdir), "latest")
    pathlib.Path(lfile).write_text(str(latest))


if __name__ == "__main__":

    mybackend = backends.default_backend()

    # Replace slow RSA_blinding_on with dummy function
    mybackend._lib.RSA_blinding_on = dummy

    ap = argparse.ArgumentParser()
    ap.add_argument("blockid", help="Blocklist ID")
    ap.add_argument("-s", "--source", nargs="?", default=".", help="Source directory")
    ap.add_argument("-d", "--dest", nargs="?", default="~/blout/", help="Destination directory")
    ap.add_argument("-f", "--force", action="store_true", help="Delete outdir if it exists")
    ap.add_argument("--fileext", default=".key", help="Key file extension (default .key)")
    ap.add_argument("--auto", action="store_true", help="Make full blocklist from json template")
    ap.add_argument("--keyrepos", default="~/keyrepos/", help="Keyrepos directory")
    ap.add_argument("-q", "--quiet", action="store_true", help="Quiet mode")

    args = ap.parse_args()

    if not args.auto:
        if args.force and os.path.exists(args.dest):
            shutil.rmtree(args.dest)
        makebl(int(args.blockid), args.source, args.dest, args.fileext)
    else:
        makefromjson(args.blockid, args.dest, args.keyrepos, args.fileext)
